{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZqHeqL23Dno",
        "outputId": "882a62fc-32e1-482f-923f-3f9da6012323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Scores:\n",
            " tensor([[0.1703, 0.1686, 0.1669, 0.1620, 0.1678, 0.1644],\n",
            "        [0.1686, 0.1703, 0.1653, 0.1636, 0.1678, 0.1644],\n",
            "        [0.1680, 0.1664, 0.1714, 0.1664, 0.1639, 0.1639],\n",
            "        [0.1631, 0.1647, 0.1664, 0.1714, 0.1655, 0.1689],\n",
            "        [0.1675, 0.1675, 0.1625, 0.1642, 0.1700, 0.1683],\n",
            "        [0.1647, 0.1647, 0.1631, 0.1680, 0.1689, 0.1706]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example sentence tokens (simulating token embeddings of size 4 for simplicity)\n",
        "token_embeddings = torch.tensor([\n",
        "    [0.1, 0.2, 0.3, 0.4],  # \"What\"\n",
        "    [0.2, 0.1, 0.4, 0.3],  # \"are\"\n",
        "    [0.3, 0.2, 0.1, 0.4],  # \"the\"\n",
        "    [0.4, 0.3, 0.2, 0.1],  # \"symptoms\"\n",
        "    [0.1, 0.3, 0.4, 0.2],  # \"of\"\n",
        "    [0.2, 0.4, 0.3, 0.1]   # \"diabetes\"\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Query, Key, and Value matrices (assuming they are the same for self-attention)\n",
        "Q = K = V = token_embeddings\n",
        "\n",
        "# Compute attention scores (QK^T)\n",
        "d_k = Q.shape[-1]  # Scaling factor (dimension of embedding)\n",
        "attention_scores = torch.matmul(Q, K.T) / (d_k ** 0.5)\n",
        "\n",
        "# Apply softmax to get attention weights\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "print(\"Self-Attention Scores:\\n\", attention_weights)\n"
      ]
    }
  ]
}